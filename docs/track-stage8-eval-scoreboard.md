# Track D: Evaluation Scoreboard + Calibration Scoring (Stage 8)

Date: 2026-02-14  
Owner worktree: `wt-eval-scoreboard`  
Owner branch prefix: `codex/eval-*` (example: `codex/eval-scoreboard-v1`)

## 1) Objective

Ship a promotion-ready evaluation layer that scores strategies on **probability quality**
(calibration + proper scoring rules), not just realized ROI.

The output must be:
- **Replayable** (offline, from saved snapshots),
- **Deterministic** (byte-stable reruns given the same inputs),
- **Spend-safe** (no Odds API usage required),
- **Actionable** (clearly tells us what to run/promote next).

This track is the canonical answer to:
> “Which strategy is best, given the data we already have?”

## 2) Why calibration scoring (and why ROI alone is not enough)

ROI over ~20–100 bets is high variance. A strategy can look great by luck even if its probability
model is wrong. Calibration scoring provides a lower-variance signal that:
- a strategy’s predicted hit probabilities align with empirical outcomes, and
- we are not systematically overconfident/underconfident.

Important nuance:
- We are scoring **the executed selection policy** (what the strategy actually picks), not a
  hypothetical model that predicts every line. This means calibration is measured on a
  selection-biased sample, but that is *still the right object* for strategy promotion.

## 3) Data available today (offline; no new credits)

We already have everything needed for settlement and calibration scoring:

### 3.1 Odds snapshots
- `parlay-data/odds_api/snapshots/<snapshot_id>/derived/*.jsonl` (and parquet mirrors)
- Each strategy report + execution plan references a concrete snapshot.

### 3.2 Outcomes (NBA context snapshots)
- `parlay-data/nba_data/context/snapshots/<snapshot_id>/results.json`
- (Optional gates) `injuries.json`, `roster.json`, and official injury PDFs when present.

### 3.3 Backtest / settlement rows
Generated by existing commands:
- `settlement.<strategy_id>.csv` and/or `backtest-results-template.<strategy_id>.csv`

Minimum fields required per graded row:
- `result` in `{win,loss,push}`
- `model_p_hit` (strategy’s predicted probability for the selected side)
- `graded_price_american` (or `selected_price_american`)

The current `backtest-summary` already computes:
- ROI, W/L/P,
- Brier (for `model_p_hit`),
- calibration buckets (binning),
- BrierLow (if `p_hit_low` exists).

This track expands that into a promotion-grade scoreboard.

## 4) Metric definitions (v1)

We define a “graded bet” as a row with `result ∈ {win, loss}` (pushes are tracked but excluded
from probability scoring rules in v1).

Let:
- `p_i` be `model_p_hit` for bet `i` (clamped to `(ε, 1-ε)` when needed),
- `y_i` be 1 for win, 0 for loss,
- `N` be number of graded bets.

### 4.1 Proper scoring rules

**Brier score** (already present):
- `brier = mean((p_i - y_i)^2)`  
Lower is better.

**Log loss** (new):
- `log_loss = -mean(y_i*log(p_i) + (1-y_i)*log(1-p_i))`  
Lower is better. Use `ε=1e-6` clamp to avoid `log(0)`.

Why add log loss if we already have Brier?
- Brier is smoother and intuitive; log loss penalizes overconfidence harder.
- Together they help detect “rare blowups” and overly sharp probabilities.

### 4.2 Calibration summaries

We already emit **calibration buckets** via fixed-width binning (default `bin_size=0.05`):
- per bin: `count`, `avg_p`, `hit_rate`, `brier`.

From those buckets we add:

**ECE (Expected Calibration Error)** (new):
- `ece = Σ_bin (count_bin / N) * abs(avg_p_bin - hit_rate_bin)`  
Lower is better.

**MCE (Max Calibration Error)** (new):
- `mce = max_bin abs(avg_p_bin - hit_rate_bin)`  
Lower is better.

Notes:
- ECE/MCE are meaningful only when bins have enough mass; we should surface the per-bin counts.
- For small samples, we prefer larger bins (e.g., `bin_size=0.10`) and a higher `min_graded`.

### 4.3 Execution realism (must-have metadata)

All scoring must be tied to the **execution settings**:
- `max_picks_per_day`
- execution-book config (`--bookmakers` / runtime whitelist)
- strategy ID + audit settings

If these are not recorded, the scoreboard is not promotion-safe.

## 5) Scoreboard artifact contract (new; promotion-ready)

### 5.1 File layout (recommendation)

Write to an analysis namespace to avoid polluting per-snapshot report folders:

- `parlay-data/reports/odds/analysis/<run_id>/aggregate-scoreboard.json`
- `parlay-data/reports/odds/analysis/<run_id>/aggregate-scoreboard.md` (optional)

Where:
- `run_id` is human-readable and stable (e.g., `eval-scoreboard-20260214-021500-et-max5`).

### 5.2 JSON schema (v1)

Top-level keys:
- `schema_version` (int)
- `generated_at_utc` (str)
- `dataset_id` (str)
- `window`:
  - `days_total` (int)
  - `days_with_any_results` (int)
  - `min_graded` (int)
  - `bin_size` (float)
  - `max_picks_per_day` (int)
- `strategies`: list of:
  - `strategy_id`
  - `rows_graded`, `wins`, `losses`, `pushes`, `roi`
  - `brier`, `log_loss`, `ece`, `mce`
  - `actionability_rate`
  - `calibration` buckets (existing structure)
  - `promotion_gate`:
    - `status` in `{pass, fail}`
    - `reasons`: stable reason codes
- `winner`:
  - `strategy_id`
  - `decision`: human-readable explanation (derived from gates + tie-breaks)

Reason codes (initial):
- `insufficient_graded`
- `missing_model_p_hit`
- `missing_prices`
- `calibration_regressed`
- `roi_negative` (optional; can be a tie-breaker rather than a hard fail)

## 6) Promotion policy (v1; conservative)

Promotion is not “highest ROI wins”; it is “eligible strategies compete”.

### 6.1 Eligibility gates

A strategy is **eligible** if:
- `rows_graded >= min_graded` (default depends on cadence; start at 50–100),
- `ece` is computed (non-null) and bins have reasonable support,
- no schema/contract failures in backtest rows.

### 6.2 Calibration floor (relative, not absolute)

Because sample sizes vary, use a *relative* rule first:
- baseline strategy = current default operational strategy (or designated baseline, e.g. `s007`)
- candidate must satisfy:
  - `ece <= baseline_ece + ece_slack`
  - `brier <= baseline_brier + brier_slack`

Use small slacks to avoid rejecting ties due to sampling noise (defaults TBD, but start with
`ece_slack=0.01`, `brier_slack=0.01`).

### 6.3 Tie-breaks (after gates)

Among eligible strategies:
1. higher ROI,
2. higher `rows_graded`,
3. lower ECE,
4. lower Brier,
5. deterministic final tie-break by `strategy_id`.

This keeps ROI relevant while preventing calibration regressions from being promoted.

## 7) Implementation plan (commit-sized slices)

### D1 — Metric primitives + tests

Add deterministic helpers:
- `log_loss(p, y)` with clamp
- `ece/mce` computed from bucket stats

Add tests for:
- edge clamps (`p=0/1`)
- perfect calibration cases
- small synthetic binning examples

Files (suggested):
- `src/prop_ev/backtest_summary.py` (extend, or extract into `src/prop_ev/eval/calibration.py`)
- `tests/test_backtest_summary.py`

### D2 — Extend backtest summary outputs

Update per-strategy summary to include:
- `log_loss`, `ece`, `mce`

Update markdown renderer table to include the new metrics (optional but useful).

Files:
- `src/prop_ev/backtest_summary.py`
- `src/prop_ev/cli.py` (`backtest-summary.md` table)
- `docs/contracts.md` (document new fields)

### D3 — Aggregate scoreboard + promotion decision

Add a command (or extend existing `strategy backtest-summarize`) to emit:
- promotion gate results per strategy
- `winner` decision explanation derived from gates + tie-breaks

Ensure the command can run in:
- `--all-complete-days --dataset-id <id>` mode (primary),
- and “single snapshot directory” mode (secondary).

### D4 — Runbook + operator workflow

Document canonical usage (no spend, offline, reproducible):
- how to choose `bin_size` and `min_graded`
- how to run multiple `max_picks_per_day` configurations (max1/max2/max5)
- how to interpret ECE vs ROI tradeoffs

Files:
- `docs/runbook.md`
- `docs/plan.md` (mark Track D slice status)

## 8) Validation protocol

Before merging:

```bash
uv run ruff format --check .
uv run ruff check .
uv run pyright
uv run pytest -q
```

Determinism check:
- run the scoreboard twice on the same fixed dataset; verify identical JSON bytes (sort keys).

## 9) Rollback

Rollback is low risk:
- existing ROI-only summary remains available,
- new fields are additive,
- promotion logic remains opt-in until explicitly used to flip defaults.

No data migration and no odds re-download required.

---

## 10) Extension: Pick-level confidence + brief explainability (Stage 7 integration)

This is an optional follow-on that makes the calibration work visible at the **pick** level
(not only at the strategy level).

### 10.0 Current implementation status (2026-02-14)

Implemented:
- `strategy backtest-summarize` can emit
  `backtest-calibration-map.json` via `--write-calibration-map`.
- `strategy backtest-summarize` can emit aggregate scoreboard artifacts under
  `reports/odds/analysis/<run_id>/aggregate-scoreboard.json` via
  `--write-analysis-scoreboard [--analysis-run-id <id>]`.
- Calibration map mode supports `walk_forward` (default) and `in_sample`.
- Pure calibration-map application utilities exist in
  `src/prop_ev/calibration_map.py`.
- Backtest summary payloads now include `schema_version` and `report_kind`
  for stable parsing.
- `generate_brief_for_snapshot` auto-loads sibling
  `backtest-calibration-map.json` (or explicit path) and annotates rows with:
  - `p_conservative`,
  - `p_calibrated`,
  - `calibration_bin`,
  - `confidence_tier`.
- Brief rendering surfaces calibrated probability in `p(hit)` as
  `X% → Y%` when calibration adjustment is available.

### 10.1 Goal

For every selected play (and optionally for every eligible candidate), surface:
- a conservative probability (`p_hit_low` when available),
- a **calibrated** probability estimate derived from historical buckets,
- an uncertainty/quality summary that an operator can understand quickly,
- an optional filter/rank policy to reduce fragile picks.

This yields:
- better “why did we take this?” explanations,
- safer defaults when we want only ~5 executable bets/day.

### 10.2 Data + leakage rules

We must avoid “using the answer to explain the answer”.

**Production/live**:
- allowed: calibration derived from a rolling historical window ending *before* the current day.

**Backtest evaluation**:
- required: walk-forward calibration.
  - for day `D`, compute calibration map from days `< D` only,
  - apply to day `D` picks.

If we can’t do walk-forward yet, we can still annotate picks with *uncertainty/quality* fields,
but calibrated probabilities must be labeled as “in-sample” and excluded from promotion logic.

### 10.3 “Calibration map” artifact (new)

Add an optional artifact produced by the scoreboard run:
- `reports/odds/analysis/<run_id>/calibration-map.json`

Recommended v1 shape:
- `schema_version`
- `generated_at_utc`
- `dataset_id`
- `bin_size`
- `strategies`: mapping `{strategy_id: {bins: [{low, high, count, avg_p, hit_rate}]}}`

This is intentionally simple (piecewise constant mapping by probability bin).

### 10.4 Pick-level fields (additive; no default behavior change)

When a calibration map is provided, annotate candidate rows with:
- `p_conservative`:
  - `p_hit_low` if present else `model_p_hit`
- `p_calibrated`:
  - map `p_conservative` to `hit_rate` of its bin
- `calibration_bin`:
  - `{low, high, count}`
- `confidence_tier` (optional):
  - derived from thresholds on `quality_score`, `uncertainty_band`, and `p_calibrated`

Keep the fields additive so older reports/briefs still parse.

### 10.5 Optional filter/rank policies (behind flags)

Do not change defaults initially. Add opt-in modes:
- `annotate`: write confidence fields only (default for first release).
- `filter`: drop picks below a floor (e.g., `p_calibrated < 0.52` or `quality_score < 0.55`).
- `rank`: rank by a conservative objective (example):
  - `score = ev_low * quality_score`
  - or `score = (p_calibrated - implied_p_execution) * (1 - uncertainty_band)`

Policy parameters must be recorded in `audit` so results are replayable.

### 10.6 Implementation slices (commit-sized)

**D5 — Produce `calibration-map.json`**
- Extend scoreboard generation to optionally emit the calibration map artifact.
- Support a walk-forward mode in dataset windows to prevent leakage.
- Tests: deterministic bytes; walk-forward uses only past days.

**D6 — Annotate picks with calibrated confidence**
- Add a pure function that applies `calibration-map.json` to candidate rows.
- Update report generation to include new per-row fields when the map is provided.
- Tests: bin assignment, missing-bin behavior, stable output.

**D7 — Brief + report rendering**
- Update brief input schema to include `p_conservative`, `p_calibrated`, `uncertainty_band`,
  and `quality_score`.
- Update PDF template to show a compact confidence line per pick.
- Default: annotate only; keep pick selection unchanged unless flags enabled.

### 10.7 “Done when”

- For a fixed dataset + snapshot set:
  - `calibration-map.json` is deterministic,
  - pick annotations are deterministic,
  - walk-forward mode reproduces the same values across reruns.
- Brief shows confidence fields without increasing report noise (no extra temp files tracked).
