# Track D: Evaluation Scoreboard + Calibration Scoring (Stage 8)

Date: 2026-02-14  
Owner worktree: `wt-eval-scoreboard`  
Owner branch prefix: `codex/eval-*` (example: `codex/eval-scoreboard-v1`)

## 1) Objective

Ship a promotion-ready evaluation layer that scores strategies on **probability quality**
(calibration + proper scoring rules), not just realized ROI.

The output must be:
- **Replayable** (offline, from saved snapshots),
- **Deterministic** (byte-stable reruns given the same inputs),
- **Spend-safe** (no Odds API usage required),
- **Actionable** (clearly tells us what to run/promote next).

This track is the canonical answer to:
> “Which strategy is best, given the data we already have?”

## 2) Why calibration scoring (and why ROI alone is not enough)

ROI over ~20–100 bets is high variance. A strategy can look great by luck even if its probability
model is wrong. Calibration scoring provides a lower-variance signal that:
- a strategy’s predicted hit probabilities align with empirical outcomes, and
- we are not systematically overconfident/underconfident.

Important nuance:
- We are scoring **the executed selection policy** (what the strategy actually picks), not a
  hypothetical model that predicts every line. This means calibration is measured on a
  selection-biased sample, but that is *still the right object* for strategy promotion.

## 3) Data available today (offline; no new credits)

We already have everything needed for settlement and calibration scoring:

### 3.1 Odds snapshots
- `parlay-data/odds_api/snapshots/<snapshot_id>/derived/*.jsonl` (and parquet mirrors)
- Each strategy report + execution plan references a concrete snapshot.

### 3.2 Outcomes (NBA context snapshots)
- `parlay-data/nba_data/context/snapshots/<snapshot_id>/results.json`
- (Optional gates) `injuries.json`, `roster.json`, and official injury PDFs when present.

### 3.3 Backtest / settlement rows
Generated by existing commands:
- `settlement.<strategy_id>.csv` and/or `backtest-results-template.<strategy_id>.csv`

Minimum fields required per graded row:
- `result` in `{win,loss,push}`
- `model_p_hit` (strategy’s predicted probability for the selected side)
- `graded_price_american` (or `selected_price_american`)

The current `backtest-summary` already computes:
- ROI, W/L/P,
- Brier (for `model_p_hit`),
- calibration buckets (binning),
- BrierLow (if `p_hit_low` exists).

This track expands that into a promotion-grade scoreboard.

## 4) Metric definitions (v1)

We define a “graded bet” as a row with `result ∈ {win, loss}` (pushes are tracked but excluded
from probability scoring rules in v1).

Let:
- `p_i` be `model_p_hit` for bet `i` (clamped to `(ε, 1-ε)` when needed),
- `y_i` be 1 for win, 0 for loss,
- `N` be number of graded bets.

### 4.1 Proper scoring rules

**Brier score** (already present):
- `brier = mean((p_i - y_i)^2)`  
Lower is better.

**Log loss** (new):
- `log_loss = -mean(y_i*log(p_i) + (1-y_i)*log(1-p_i))`  
Lower is better. Use `ε=1e-6` clamp to avoid `log(0)`.

Why add log loss if we already have Brier?
- Brier is smoother and intuitive; log loss penalizes overconfidence harder.
- Together they help detect “rare blowups” and overly sharp probabilities.

### 4.2 Calibration summaries

We already emit **calibration buckets** via fixed-width binning (default `bin_size=0.05`):
- per bin: `count`, `avg_p`, `hit_rate`, `brier`.

From those buckets we add:

**ECE (Expected Calibration Error)** (new):
- `ece = Σ_bin (count_bin / N) * abs(avg_p_bin - hit_rate_bin)`  
Lower is better.

**MCE (Max Calibration Error)** (new):
- `mce = max_bin abs(avg_p_bin - hit_rate_bin)`  
Lower is better.

Notes:
- ECE/MCE are meaningful only when bins have enough mass; we should surface the per-bin counts.
- For small samples, we prefer larger bins (e.g., `bin_size=0.10`) and a higher `min_graded`.

### 4.3 Execution realism (must-have metadata)

All scoring must be tied to the **execution settings**:
- `max_picks_per_day`
- execution-book config (`--bookmakers` / runtime whitelist)
- strategy ID + audit settings

If these are not recorded, the scoreboard is not promotion-safe.

## 5) Scoreboard artifact contract (new; promotion-ready)

### 5.1 File layout (recommendation)

Write to an analysis namespace to avoid polluting per-snapshot report folders:

- `parlay-data/reports/odds/analysis/<run_id>/aggregate-scoreboard.json`
- `parlay-data/reports/odds/analysis/<run_id>/aggregate-scoreboard.md` (optional)

Where:
- `run_id` is human-readable and stable (e.g., `eval-scoreboard-20260214-021500-et-max5`).

### 5.2 JSON schema (v1)

Top-level keys:
- `schema_version` (int)
- `generated_at_utc` (str)
- `dataset_id` (str)
- `window`:
  - `days_total` (int)
  - `days_with_any_results` (int)
  - `min_graded` (int)
  - `bin_size` (float)
  - `max_picks_per_day` (int)
- `strategies`: list of:
  - `strategy_id`
  - `rows_graded`, `wins`, `losses`, `pushes`, `roi`
  - `brier`, `log_loss`, `ece`, `mce`
  - `actionability_rate`
  - `calibration` buckets (existing structure)
  - `promotion_gate`:
    - `status` in `{pass, fail}`
    - `reasons`: stable reason codes
- `winner`:
  - `strategy_id`
  - `decision`: human-readable explanation (derived from gates + tie-breaks)

Reason codes (initial):
- `insufficient_graded`
- `missing_model_p_hit`
- `missing_prices`
- `calibration_regressed`
- `roi_negative` (optional; can be a tie-breaker rather than a hard fail)

## 6) Promotion policy (v1; conservative)

Promotion is not “highest ROI wins”; it is “eligible strategies compete”.

### 6.1 Eligibility gates

A strategy is **eligible** if:
- `rows_graded >= min_graded` (default depends on cadence; start at 50–100),
- `ece` is computed (non-null) and bins have reasonable support,
- no schema/contract failures in backtest rows.

### 6.2 Calibration floor (relative, not absolute)

Because sample sizes vary, use a *relative* rule first:
- baseline strategy = current default operational strategy (or designated baseline, e.g. `s007`)
- candidate must satisfy:
  - `ece <= baseline_ece + ece_slack`
  - `brier <= baseline_brier + brier_slack`

Use small slacks to avoid rejecting ties due to sampling noise (defaults TBD, but start with
`ece_slack=0.01`, `brier_slack=0.01`).

### 6.3 Tie-breaks (after gates)

Among eligible strategies:
1. higher ROI,
2. higher `rows_graded`,
3. lower ECE,
4. lower Brier,
5. deterministic final tie-break by `strategy_id`.

This keeps ROI relevant while preventing calibration regressions from being promoted.

## 7) Implementation plan (commit-sized slices)

### D1 — Metric primitives + tests

Add deterministic helpers:
- `log_loss(p, y)` with clamp
- `ece/mce` computed from bucket stats

Add tests for:
- edge clamps (`p=0/1`)
- perfect calibration cases
- small synthetic binning examples

Files (suggested):
- `src/prop_ev/backtest_summary.py` (extend, or extract into `src/prop_ev/eval/calibration.py`)
- `tests/test_backtest_summary.py`

### D2 — Extend backtest summary outputs

Update per-strategy summary to include:
- `log_loss`, `ece`, `mce`

Update markdown renderer table to include the new metrics (optional but useful).

Files:
- `src/prop_ev/backtest_summary.py`
- `src/prop_ev/cli.py` (`backtest-summary.md` table)
- `docs/contracts.md` (document new fields)

### D3 — Aggregate scoreboard + promotion decision

Add a command (or extend existing `strategy backtest-summarize`) to emit:
- promotion gate results per strategy
- `winner` decision explanation derived from gates + tie-breaks

Ensure the command can run in:
- `--all-complete-days --dataset-id <id>` mode (primary),
- and “single snapshot directory” mode (secondary).

### D4 — Runbook + operator workflow

Document canonical usage (no spend, offline, reproducible):
- how to choose `bin_size` and `min_graded`
- how to run multiple `max_picks_per_day` configurations (max1/max2/max5)
- how to interpret ECE vs ROI tradeoffs

Files:
- `docs/runbook.md`
- `docs/plan.md` (mark Track D slice status)

## 8) Validation protocol

Before merging:

```bash
uv run ruff format --check .
uv run ruff check .
uv run pyright
uv run pytest -q
```

Determinism check:
- run the scoreboard twice on the same fixed dataset; verify identical JSON bytes (sort keys).

## 9) Rollback

Rollback is low risk:
- existing ROI-only summary remains available,
- new fields are additive,
- promotion logic remains opt-in until explicitly used to flip defaults.

No data migration and no odds re-download required.

